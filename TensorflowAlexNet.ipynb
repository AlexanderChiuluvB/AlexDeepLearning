{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorflowAlexNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexanderChiuluvB/AlexDeepLearning/blob/master/TensorflowAlexNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SosMrOBED1GM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from datetime import datetime\n",
        "import tensorflow\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "esIADvh3FPdl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size=32\n",
        "num_batches = 100\n",
        "def printActivation(t):\n",
        "  \"\"\"\n",
        "  input: tensor t\n",
        "  \n",
        "  \"\"\"\n",
        "  \n",
        "  print(t.op.name,' ',t.get_shape().as_list())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j4lPwtHCOS8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fcLayer(x,inputD,outputD,reluFlag,name):\n",
        "  \"\"\"Fully Connected\"\"\"\n",
        "  with tf.variable_scope(name) as scope:\n",
        "    w = tf.get_variable(\"w\",shape=[inputD,outputD],dtype=\"float\")\n",
        "    b = tf.get_variable(\"b\",[outputD],dtype=\"float\")\n",
        "    out = tf.nn.xw_plus_b(x,w,b,name=scope.name)\n",
        "    if reluFlag:\n",
        "      return tf.nn.relu(out)\n",
        "    else:\n",
        "      return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RMmNEKz1FPlf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#网络结构\n",
        "\n",
        "def inference(images):\n",
        "  \n",
        "  parameters = []\n",
        "  #第一层卷积层\n",
        "  with tf.name_scope('conv1') as scope:\n",
        "    #初始化内核\n",
        "    #tf.truncated_normal　截取的正态分布函数，用于初始化卷积核的参数kernel，卷积核尺寸11x11,颜色通道为３，卷积核数量为64\n",
        "    kernel = tf.Variable(tf.truncated_normal([11,11,3,64],dtype=tf.float32,stddev=1e-1),name='weights')\n",
        "    #conv2d　strides参数是一个长度为４的列表，[1,x,y,1] x,y,分别指横向和纵向的步长，第一个参数是ｂａｔｃｈ个数，第４个参数是卷积的深度，两者通常都设置为１\n",
        "    conv = tf.nn.conv2d(images,kernel,[1,4,4,1],padding='SAME')\n",
        "    #bias 的ｓｈａｐｅ等于该层的输出层维数\n",
        "    biases = tf.Variable(tf.constant(0.0,shape=[64],dtype=tf.float32),trainable=True,name='biases')\n",
        "    bias = tf.nn.bias_add(conv,biases)\n",
        "    conv1 = tf.nn.relu(bias,name=scope)\n",
        "    printActivation(conv1)\n",
        "    \n",
        "    parameters+=[kernel,biases]\n",
        "    #AlexNet 使用最大池化而不是平均池化，避免了模糊\n",
        "    #kernel size = 3x3 就是把3x3的像素块降为1x1的像素块 padding='VALID'，取样的时候不能超过边框，而'SAME'的时候可以填充边界外的点\n",
        "    pool1 = tf.nn.max_pool(conv1,ksize=[1,3,3,1],strides=[1,2,2,1],padding='VALID',name='pool1')\n",
        "    \n",
        "    #printActivation(pool1)\n",
        "    \n",
        "    \n",
        "  with tf.name_scope('conv2') as scope:\n",
        "    \n",
        "    #卷积核尺寸为5x5，输入通道数（也就是上一层的输出通道数，也是上一层的卷积核数量）＝６４，这一层的卷积核数目为１９２\n",
        "    kernel = tf.Variable(tf.truncated_normal([5,5,64,192],dtype=tf.float32,stddev=1e-1),name='weights')\n",
        "\n",
        "    conv = tf.nn.conv2d(pool1,kernel,[1,1,1,1],padding='SAME')\n",
        "    \n",
        "    biases = tf.Variable(tf.constant(0.0,shape=[192],dtype=tf.float32),trainable=True,name='biases')\n",
        "    bias = tf.nn.bias_add(conv,biases)\n",
        "    conv2 = tf.nn.relu(bias,name=scope)\n",
        "   \n",
        "    \n",
        "    parameters+=[kernel,biases]\n",
        "    #AlexNet 使用最大池化而不是平均池化，避免了模糊\n",
        "    #kernel size = 3x3 就是把3x3的像素块降为1x1的像素块 padding='VALID'，取样的时候不能超过边框，而'SAME'的时候可以填充边界外的点\n",
        "    pool2 = tf.nn.max_pool(conv2,ksize=[1,3,3,1],strides=[1,2,2,1],padding='VALID',name='pool2')\n",
        "    \n",
        "    \n",
        "  with tf.name_scope('conv3') as scope:\n",
        "    \n",
        "    kernel = tf.Variable(tf.truncated_normal([3,3,192,384],dtype=tf.float32,stddev=1e-1),name='weights')\n",
        "    conv = tf.nn.conv2d(pool2,kernel,[1,1,1,1],padding='SAME')\n",
        "    biases = tf.Variable(tf.constant(0.0,shape=[384],dtype=tf.float32),trainable=True,name='biases')\n",
        "    bias = tf.nn.bias_add(conv,biases)\n",
        "    conv3 = tf.nn.relu(bias,name=scope)\n",
        "    \n",
        "    parameters+=[kernel,biases]\n",
        "    \n",
        "  with tf.name_scope('conv4') as scope:\n",
        "    \n",
        "    kernel = tf.Variable(tf.truncated_normal([3,3,384,256],dtype=tf.float32,stddev=1e-1),name='weights')\n",
        "    conv = tf.nn.conv2d(conv3,kernel,[1,1,1,1],padding='SAME')\n",
        "    biases = tf.Variable(tf.constant(0.0,shape=[256],dtype=tf.float32),trainable=True,name='biases')\n",
        "    bias = tf.nn.bias_add(conv,biases)\n",
        "    conv4 = tf.nn.relu(bias,name=scope)\n",
        "    parameters+=[kernel,biases]\n",
        "    \n",
        "  with tf.name_scope('conv5') as scope:\n",
        "    \n",
        "    kernel = tf.Variable(tf.truncated_normal([3,3,256,256],dtype=tf.float32,stddev=1e-1),name='weights')\n",
        "    conv = tf.nn.conv2d(conv4,kernel,[1,1,1,1],padding='SAME')\n",
        "    biases = tf.Variable(tf.constant(0.0,shape=[256],dtype=tf.float32),trainable=True,name='biases')\n",
        "    bias = tf.nn.bias_add(conv,biases)\n",
        "    conv5 = tf.nn.relu(bias,name=scope)\n",
        "    parameters+=[kernel,biases]\n",
        "    \n",
        "    pool5 = tf.nn.max_pool(conv5,ksize=[1,3,3,1],strides=[1,2,2,1],padding='VALID',name='pool5')\n",
        "    \n",
        "    fcInput = tf.reshape(pool5,[-1,256*6*6])\n",
        "    fc1 = fcLayer(fcInput,256*6*6,4096,True,\"fc6\")\n",
        "    dropout1 = tf.nn.dropout(fc1,0.5)\n",
        "    \n",
        "    fc2 = fcLayer(dropout1,4096,4096,True,\"fc7\")\n",
        "    dropout2 = tf.nn.dropout(fc2,0.5)\n",
        "    \n",
        "    #here 1000 means self.classnumber\n",
        "    fc3 = fcLayer(dropout2,4096,1000,True,\"fc8\")\n",
        "    \n",
        "    \n",
        "  return fc3,parameters\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUudbnHZR1gq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def time_tensorflow_run(session, target, info_string):\n",
        "#  \"\"\"Run the computation to obtain the target tensor and print timing stats.\n",
        "#\n",
        "#  Args:\n",
        "#    session: the TensorFlow session to run the computation under.\n",
        "#    target: the target Tensor that is passed to the session's run() function.\n",
        "#    info_string: a string summarizing this run, to be printed with the stats.\n",
        "#\n",
        "#  Returns:\n",
        "#    None\n",
        "#  \"\"\"\n",
        "    num_steps_burn_in = 10\n",
        "    total_duration = 0.0\n",
        "    total_duration_squared = 0.0\n",
        "    for i in range(num_batches + num_steps_burn_in):\n",
        "        start_time = time.time()\n",
        "        _ = session.run(target)\n",
        "        duration = time.time() - start_time\n",
        "        if i >= num_steps_burn_in:\n",
        "            if not i % 10:\n",
        "                print ('%s: step %d, duration = %.3f' %\n",
        "                       (datetime.now(), i - num_steps_burn_in, duration))\n",
        "            total_duration += duration\n",
        "            total_duration_squared += duration * duration\n",
        "    mn = total_duration / num_batches\n",
        "    vr = total_duration_squared / num_batches - mn * mn\n",
        "    sd = math.sqrt(vr)\n",
        "    print ('%s: %s across %d steps, %.3f +/- %.3f sec / batch' %\n",
        "           (datetime.now(), info_string, num_batches, mn, sd))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "upRs07BmFPre",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#running!\n",
        "def run():\n",
        "  with tf.Graph().as_default():\n",
        "    image_size=224\n",
        "    images = tf.Variable(tf.random_normal([batch_size,image_size,image_size,3],dtype=tf.float32,\n",
        "                                         stddev=1e-1))\n",
        "    fc3,parameters = inference(images)\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "    time_tensorflow_run(sess,fc3,\"Forward\")\n",
        "    objective = tf.nn.l2_loss(fc3)\n",
        "    grad = tf.gradients(objective,parameters)\n",
        "    time_tensorflow_run(sess,grad,\"Forward-backward\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lCdnEdt5Qmwr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-qEpDrv1FPwR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}