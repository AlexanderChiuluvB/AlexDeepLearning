{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "[2019-03-14 11:55:00,413] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n",
      "14.0\n",
      "18.0\n",
      "78.0\n",
      "34.0\n",
      "22.0\n",
      "15.0\n",
      "31.0\n",
      "17.0\n",
      "14.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "random_episodes = 0\n",
    "reward_sum = 0\n",
    "while random_episodes<10:\n",
    "    env.render()\n",
    "    observation,reward,done,_ = env.step(np.random.randint(0,2))\n",
    "    reward_sum+=reward\n",
    "    if done:\n",
    "        random_episodes+=1\n",
    "        print(reward_sum)\n",
    "        reward_sum = 0\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 10 #hidden layer\n",
    "batch_size = 5\n",
    "learning_rate = 1e-2\n",
    "D=4\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##MLP\n",
    "observations= tf.placeholder(tf.float32,[None,D],name='input_X')\n",
    "W1 = tf.get_variable(\"W1\",shape=[D,H],initializer = tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations,W1))\n",
    "W2 = tf.get_variable(\"W2\",shape=[H,1],initializer = tf.contrib.layers.xavier_initializer())\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2))\n",
    "score =  tf.matmul(layer1,W2)\n",
    "probability = tf.nn.sigmoid(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#人工设置的虚拟label，二值化\n",
    "input_y = tf.placeholder(tf.float32,[None,1],name='input_y')\n",
    "#每个Acation 的潜在价值\n",
    "advantages = tf.placeholder(tf.float32,name='reward_signal')\n",
    "loglik = tf.log(input_y*(input_y-probability)+(1-input_y)*(input_y+probability))\n",
    "#loglik: 当前Action对应概率的对数，与潜在价值相乘，并取负数作为损失\n",
    "loss = -tf.reduce_mean(loglik*advantages)\n",
    "#所有可以训练的参数\n",
    "tvars = tf.trainable_variables()\n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "W1Grad = tf.placeholder(tf.float32,name=\"batch_grad1\")\n",
    "W2Grad = tf.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W2Grad]\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#定义函数discount_rewards\n",
    "#用来估算Action对应的潜在价值discount_r\n",
    "#每一个Action的潜在价值，为后一个Action的潜在价值乘以衰减系数gamma再加上其直接获得的reward\n",
    "\n",
    "\n",
    "#越靠前的潜在价值会越大\n",
    "def discount_rewards(r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_rewards = 0\n",
    "    size = r.size\n",
    "    for t in reversed(range(size)):\n",
    "        running_rewards = gamma*running_rewards+r[t]\n",
    "        discounted_r[t] = running_rewards\n",
    "    return discounted_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alex/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-03-14 11:55:12,910] From /home/alex/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 19.400000.  Total average reward 19.400000.\n",
      "Average reward for episode 15.400000.  Total average reward 19.360000.\n",
      "Average reward for episode 15.800000.  Total average reward 19.324400.\n",
      "Average reward for episode 18.800000.  Total average reward 19.319156.\n",
      "Average reward for episode 22.400000.  Total average reward 19.349964.\n",
      "Average reward for episode 15.400000.  Total average reward 19.310465.\n",
      "Average reward for episode 13.800000.  Total average reward 19.255360.\n",
      "Average reward for episode 12.400000.  Total average reward 19.186807.\n",
      "Average reward for episode 21.800000.  Total average reward 19.212938.\n",
      "Average reward for episode 16.200000.  Total average reward 19.182809.\n",
      "Average reward for episode 23.200000.  Total average reward 19.222981.\n",
      "Average reward for episode 11.800000.  Total average reward 19.148751.\n",
      "Average reward for episode 15.800000.  Total average reward 19.115264.\n",
      "Average reward for episode 22.600000.  Total average reward 19.150111.\n",
      "Average reward for episode 21.200000.  Total average reward 19.170610.\n",
      "Average reward for episode 17.000000.  Total average reward 19.148904.\n",
      "Average reward for episode 12.600000.  Total average reward 19.083415.\n",
      "Average reward for episode 22.600000.  Total average reward 19.118581.\n",
      "Average reward for episode 17.200000.  Total average reward 19.099395.\n",
      "Average reward for episode 16.200000.  Total average reward 19.070401.\n",
      "Average reward for episode 12.600000.  Total average reward 19.005697.\n",
      "Average reward for episode 17.200000.  Total average reward 18.987640.\n",
      "Average reward for episode 19.200000.  Total average reward 18.989764.\n",
      "Average reward for episode 19.800000.  Total average reward 18.997866.\n",
      "Average reward for episode 16.600000.  Total average reward 18.973887.\n",
      "Average reward for episode 16.600000.  Total average reward 18.950148.\n",
      "Average reward for episode 20.000000.  Total average reward 18.960647.\n",
      "Average reward for episode 11.600000.  Total average reward 18.887040.\n",
      "Average reward for episode 12.200000.  Total average reward 18.820170.\n",
      "Average reward for episode 20.600000.  Total average reward 18.837968.\n",
      "Average reward for episode 16.600000.  Total average reward 18.815589.\n",
      "Average reward for episode 15.600000.  Total average reward 18.783433.\n",
      "Average reward for episode 16.600000.  Total average reward 18.761598.\n",
      "Average reward for episode 24.000000.  Total average reward 18.813982.\n",
      "Average reward for episode 15.000000.  Total average reward 18.775843.\n",
      "Average reward for episode 15.200000.  Total average reward 18.740084.\n",
      "Average reward for episode 23.000000.  Total average reward 18.782683.\n",
      "Average reward for episode 19.600000.  Total average reward 18.790856.\n",
      "Average reward for episode 14.000000.  Total average reward 18.742948.\n",
      "Average reward for episode 24.000000.  Total average reward 18.795518.\n",
      "Average reward for episode 17.400000.  Total average reward 18.781563.\n",
      "Average reward for episode 20.000000.  Total average reward 18.793748.\n",
      "Average reward for episode 13.600000.  Total average reward 18.741810.\n",
      "Average reward for episode 12.400000.  Total average reward 18.678392.\n",
      "Average reward for episode 18.600000.  Total average reward 18.677608.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-002b211a1583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtfprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtfprob\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#分别是环境信息observation的列表，定义的label的列表，每一个Action的Reward\n",
    "xs,ys,drs = [],[],[]\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "running_reward = None\n",
    "total_episodes = 10000\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()\n",
    "    \n",
    "    gradBuffer = sess.run(tvars)\n",
    "    \n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad*0\n",
    "        \n",
    "    while episode_number <= total_episodes:\n",
    "        \n",
    "        #如果batch的平均reward大于1000的时候，调用环境来进行展示\n",
    "        \n",
    "        if reward_sum/batch_size > 100 or rendering == True:\n",
    "            \n",
    "            env.render()\n",
    "            rendering = True\n",
    "        x = np.reshape(observation,[1,D])\n",
    "        \n",
    "        tfprob = sess.run(probability,feed_dict={observations:x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "        \n",
    "        \n",
    "        xs.append(x)\n",
    "        y = 1 - action\n",
    "        ys.append(y)\n",
    "        \n",
    "        observation,reward,done,info = env.step(action)\n",
    "        reward_sum += reward\n",
    "        \n",
    "        drs.append(reward)\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            episode_number+=1\n",
    "            #纵向堆叠\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            xs,ys,drs = [],[],[]\n",
    "\n",
    "            #normalized to a stable distribution\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr /= np.std(discounted_epr) \n",
    "\n",
    "\n",
    "            tGrad = sess.run(newGrads,feed_dict={observations:epx,input_y:epy,advantages:epr})\n",
    "\n",
    "            for ix,grad in enumerate(tGrad):\n",
    "                gradBuffer[ix]+=grad\n",
    "\n",
    "                \n",
    "            if episode_number % batch_size==0:\n",
    "                sess.run(updateGrads,feed_dict={W1Grad:gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                \n",
    "                for ix,grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad*0\n",
    "                \n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                print ('Average reward for episode %f.  Total average reward %f.' % (reward_sum/batch_size, running_reward/batch_size))\n",
    "                \n",
    "                if reward_sum/batch_size>200:\n",
    "                    print(\"Task solved in \",episode_number,\"episodes!\")\n",
    "                    break\n",
    "                \n",
    "                reward_sum = 0\n",
    "                \n",
    "            observation = env.reset()\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode 15.400000.  Total average reward 15.400000.\n",
      "Average reward for episode 21.200000.  Total average reward 15.458000.\n",
      "Average reward for episode 24.000000.  Total average reward 15.543420.\n",
      "Average reward for episode 29.800000.  Total average reward 15.685986.\n",
      "Average reward for episode 38.000000.  Total average reward 15.909126.\n",
      "Average reward for episode 18.400000.  Total average reward 15.934035.\n",
      "Average reward for episode 27.200000.  Total average reward 16.046694.\n",
      "Average reward for episode 18.800000.  Total average reward 16.074227.\n",
      "Average reward for episode 36.000000.  Total average reward 16.273485.\n",
      "Average reward for episode 24.800000.  Total average reward 16.358750.\n",
      "Average reward for episode 21.000000.  Total average reward 16.405163.\n",
      "Average reward for episode 36.800000.  Total average reward 16.609111.\n",
      "Average reward for episode 27.600000.  Total average reward 16.719020.\n",
      "Average reward for episode 26.600000.  Total average reward 16.817830.\n",
      "Average reward for episode 49.000000.  Total average reward 17.139652.\n",
      "Average reward for episode 16.400000.  Total average reward 17.132255.\n",
      "Average reward for episode 26.800000.  Total average reward 17.228932.\n",
      "Average reward for episode 33.000000.  Total average reward 17.386643.\n",
      "Average reward for episode 52.200000.  Total average reward 17.734777.\n",
      "Average reward for episode 40.000000.  Total average reward 17.957429.\n",
      "Average reward for episode 49.800000.  Total average reward 18.275855.\n",
      "Average reward for episode 20.000000.  Total average reward 18.293096.\n",
      "Average reward for episode 48.800000.  Total average reward 18.598165.\n",
      "Average reward for episode 24.200000.  Total average reward 18.654183.\n",
      "Average reward for episode 43.800000.  Total average reward 18.905642.\n",
      "Average reward for episode 25.600000.  Total average reward 18.972585.\n",
      "Average reward for episode 25.400000.  Total average reward 19.036859.\n",
      "Average reward for episode 42.200000.  Total average reward 19.268491.\n",
      "Average reward for episode 37.200000.  Total average reward 19.447806.\n",
      "Average reward for episode 27.200000.  Total average reward 19.525328.\n",
      "Average reward for episode 32.400000.  Total average reward 19.654075.\n",
      "Average reward for episode 39.200000.  Total average reward 19.849534.\n",
      "Average reward for episode 39.400000.  Total average reward 20.045038.\n",
      "Average reward for episode 34.200000.  Total average reward 20.186588.\n",
      "Average reward for episode 25.200000.  Total average reward 20.236722.\n",
      "Average reward for episode 40.200000.  Total average reward 20.436355.\n",
      "Average reward for episode 40.000000.  Total average reward 20.631991.\n",
      "Average reward for episode 29.800000.  Total average reward 20.723672.\n",
      "Average reward for episode 39.200000.  Total average reward 20.908435.\n",
      "Average reward for episode 41.600000.  Total average reward 21.115350.\n",
      "Average reward for episode 46.200000.  Total average reward 21.366197.\n",
      "Average reward for episode 56.600000.  Total average reward 21.718535.\n",
      "Average reward for episode 39.600000.  Total average reward 21.897350.\n",
      "Average reward for episode 55.800000.  Total average reward 22.236376.\n",
      "Average reward for episode 42.400000.  Total average reward 22.438012.\n",
      "Average reward for episode 40.600000.  Total average reward 22.619632.\n",
      "Average reward for episode 32.400000.  Total average reward 22.717436.\n",
      "Average reward for episode 39.600000.  Total average reward 22.886262.\n",
      "Average reward for episode 66.800000.  Total average reward 23.325399.\n",
      "Average reward for episode 50.400000.  Total average reward 23.596145.\n",
      "Average reward for episode 62.200000.  Total average reward 23.982184.\n",
      "Average reward for episode 51.600000.  Total average reward 24.258362.\n",
      "Average reward for episode 61.000000.  Total average reward 24.625778.\n",
      "Average reward for episode 40.200000.  Total average reward 24.781520.\n",
      "Average reward for episode 45.400000.  Total average reward 24.987705.\n",
      "Average reward for episode 40.800000.  Total average reward 25.145828.\n",
      "Average reward for episode 31.800000.  Total average reward 25.212370.\n",
      "Average reward for episode 46.400000.  Total average reward 25.424246.\n",
      "Average reward for episode 40.200000.  Total average reward 25.572004.\n",
      "Average reward for episode 54.800000.  Total average reward 25.864284.\n",
      "Average reward for episode 67.400000.  Total average reward 26.279641.\n",
      "Average reward for episode 61.400000.  Total average reward 26.630844.\n",
      "Average reward for episode 50.600000.  Total average reward 26.870536.\n",
      "Average reward for episode 66.800000.  Total average reward 27.269831.\n",
      "Average reward for episode 42.200000.  Total average reward 27.419132.\n",
      "Average reward for episode 63.200000.  Total average reward 27.776941.\n",
      "Average reward for episode 50.000000.  Total average reward 27.999171.\n",
      "Average reward for episode 66.600000.  Total average reward 28.385180.\n",
      "Average reward for episode 58.200000.  Total average reward 28.683328.\n",
      "Average reward for episode 54.400000.  Total average reward 28.940495.\n",
      "Average reward for episode 54.800000.  Total average reward 29.199090.\n",
      "Average reward for episode 61.200000.  Total average reward 29.519099.\n",
      "Average reward for episode 64.200000.  Total average reward 29.865908.\n",
      "Average reward for episode 52.800000.  Total average reward 30.095249.\n",
      "Average reward for episode 62.400000.  Total average reward 30.418296.\n",
      "Average reward for episode 47.800000.  Total average reward 30.592113.\n",
      "Average reward for episode 52.600000.  Total average reward 30.812192.\n",
      "Average reward for episode 84.400000.  Total average reward 31.348070.\n",
      "Average reward for episode 58.400000.  Total average reward 31.618590.\n",
      "Average reward for episode 60.600000.  Total average reward 31.908404.\n",
      "Average reward for episode 64.000000.  Total average reward 32.229320.\n",
      "Average reward for episode 87.000000.  Total average reward 32.777026.\n",
      "Average reward for episode 64.400000.  Total average reward 33.093256.\n",
      "Average reward for episode 94.200000.  Total average reward 33.704324.\n",
      "Average reward for episode 67.600000.  Total average reward 34.043280.\n",
      "Average reward for episode 66.400000.  Total average reward 34.366848.\n",
      "Average reward for episode 79.000000.  Total average reward 34.813179.\n",
      "Average reward for episode 70.800000.  Total average reward 35.173047.\n",
      "Average reward for episode 52.800000.  Total average reward 35.349317.\n",
      "Average reward for episode 56.600000.  Total average reward 35.561824.\n",
      "Average reward for episode 75.000000.  Total average reward 35.956205.\n",
      "Average reward for episode 128.400000.  Total average reward 36.880643.\n",
      "Average reward for episode 82.200000.  Total average reward 37.333837.\n",
      "Average reward for episode 83.000000.  Total average reward 37.790499.\n",
      "Average reward for episode 67.600000.  Total average reward 38.088594.\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: <class 'TypeError'>: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c2d1cd401933>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# so let's only look at it once our agent is doing a good job.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreward_sum\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrendering\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mrendering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36mdispatch_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;31m# Check for the events specific to this window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         while xlib.XCheckWindowEvent(_x_display, _window,\n\u001b[0;32m--> 875\u001b[0;31m                                      0x1ffffff, byref(e)):\n\u001b[0m\u001b[1;32m    876\u001b[0m             \u001b[0;31m# Key events are filtered by the xlib window event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# handler so they get a shot at the prefiltered event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument 2: <class 'TypeError'>: wrong type"
     ]
    }
   ],
   "source": [
    "xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "total_episodes = 10000\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset() # Obtain an initial observation of the environment\n",
    "\n",
    "    # Reset the gradient placeholder. We will collect gradients in \n",
    "    # gradBuffer until we are ready to update our policy network. \n",
    "    gradBuffer = sess.run(tvars)\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    \n",
    "    while episode_number <= total_episodes:\n",
    "        \n",
    "        # Rendering the environment slows things down, \n",
    "        # so let's only look at it once our agent is doing a good job.\n",
    "        #if reward_sum/batch_size > 100 or rendering == True : \n",
    "            #env.render()\n",
    "            #rendering = True\n",
    "            \n",
    "        # Make sure the observation is in a shape the network can handle.\n",
    "        x = np.reshape(observation,[1,D])\n",
    "        \n",
    "        # Run the policy network and get an action to take. \n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "        \n",
    "        xs.append(x) # observation\n",
    "        y = 1 if action == 0 else 0 # a \"fake label\"\n",
    "        ys.append(y)\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "        # 批量更新\n",
    "        if done: \n",
    "            episode_number += 1\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            tfp = tfps\n",
    "            xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[] # reset array memory\n",
    "\n",
    "            # compute the discounted reward backwards through time\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            # size the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "            discounted_epr -= np.mean(discounted_epr)\n",
    "            discounted_epr /= np.std(discounted_epr)\n",
    "            \n",
    "            # Get the gradient for this episode, and save it in the gradBuffer\n",
    "            tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "            for ix,grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "                \n",
    "            # If we have completed enough episodes, then update the policy network with our gradients.\n",
    "            if episode_number % batch_size == 0: \n",
    "                #当累积了足够多的梯度的时候才才进行梯度的更新\n",
    "                sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W2Grad:gradBuffer[1]})\n",
    "                for ix,grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                # Give a summary of how well our network is doing for each batch of episodes.\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                print ('Average reward for episode %f.  Total average reward %f.' % (reward_sum/batch_size, running_reward/batch_size))\n",
    "                \n",
    "                if reward_sum/batch_size > 200: \n",
    "                    print (\"Task solved in\",episode_number,'episodes!')\n",
    "                    break\n",
    "                    \n",
    "                reward_sum = 0\n",
    "            \n",
    "            observation = env.reset()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-03-14 13:27:38,354] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.62426\n",
      "Final Q-Table Values\n",
      "[[5.57836375e-01 1.13728739e-02 1.01533036e-02 1.12691614e-02]\n",
      " [2.61223193e-04 8.35631695e-03 1.80985851e-03 6.02301528e-01]\n",
      " [8.14432174e-03 1.63941675e-02 6.06488748e-03 6.47034787e-01]\n",
      " [3.64013696e-04 2.36993606e-03 1.36648726e-03 3.72670831e-01]\n",
      " [6.63660195e-01 1.71008614e-03 1.88394561e-03 1.40874090e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.18207494e-06 3.66533616e-08 8.10999621e-02 9.21918470e-08]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.34395835e-04 2.06623395e-04 2.24706076e-03 8.11000456e-01]\n",
      " [0.00000000e+00 7.95460759e-01 0.00000000e+00 0.00000000e+00]\n",
      " [5.80904729e-01 1.86760613e-05 1.38810363e-05 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.13893027e-03 3.49952384e-04 9.21441376e-01 4.86602202e-03]\n",
      " [0.00000000e+00 9.93345690e-01 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#下面是一个Ｑ估值网络的ｄｅｍｏ\n",
    "\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "lr = .85\n",
    "y = 0.99\n",
    "num_episodes = 100000\n",
    "rewardList = []\n",
    "for i in range(num_episodes):\n",
    "    s = env.reset()\n",
    "    reward_sum = 0\n",
    "    done = False\n",
    "    j = 0\n",
    "    while j<99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:]+np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        new_status,reward,done,_ = env.step(a)\n",
    "        \n",
    "        #update status\n",
    "        Q[s,a] = (1-lr)*Q[s,a] + lr*(reward+y*np.max(Q[new_status,:]))\n",
    "        reward_sum+=reward\n",
    "        s = new_status\n",
    "        if done==True:\n",
    "            break\n",
    "    rewardList.append(reward_sum)\n",
    "print (\"Score over time: \" +  str(sum(rewardList)/num_episodes))\n",
    "print (\"Final Q-Table Values\")\n",
    "print (Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
